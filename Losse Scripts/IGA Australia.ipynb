{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bbba1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 105.0.5195\n",
      "Get LATEST chromedriver version for 105.0.5195 google-chrome\n",
      "Driver [C:\\Users\\verbr\\.wdm\\drivers\\chromedriver\\win32\\105.0.5195.52\\chromedriver.exe] found in cache\n",
      "C:\\Users\\verbr\\AppData\\Local\\Temp/ipykernel_2036/1424107507.py:5: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import csv\n",
    "from time import sleep\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03c796c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "res = driver.page_source.encode('utf-8')\n",
    "soup = BeautifulSoup(res, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3219f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_page(retailer, url):    \n",
    "    \n",
    "  \n",
    "    #call get_url function to get the correct url linked to the Region that is going to be scraped    \n",
    "    sleep(1)\n",
    "    \n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    \n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "    sleep(10)\n",
    "    #create scraping_round variable that counts the amount of times all the wines are being scraped \n",
    "    scraping_round = 1\n",
    "    lengte = 0\n",
    "    num_products_view_2 = 0\n",
    "    #create a scroller that scrolls the_range amount of times.     \n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "        \n",
    "    # total number of products in current view\n",
    "    num_products_view_1 = int(len(soup.find_all(attrs={\"class\": \"ColListing--1fk1zey dlJvmX\"})))\n",
    "        \n",
    "    #create attributes the_name_id, the_price_id, the_brand_id and the_volume_id \n",
    "    for counter in range(num_products_view_2, num_products_view_1):\n",
    "        the_name_id = soup.find_all(attrs={\"class\": \"ColListing--1fk1zey dlJvmX\"})[counter].find_all(attrs={\"class\": \"ProductCardTitle--1qhpid2 kugdfA\"})[0].text\n",
    "        the_name_id = the_name_id[:-24]\n",
    "        print(the_name_id)\n",
    "        try: \n",
    "            the_price_id = soup.find_all(attrs={\"class\": \"ColListing--1fk1zey dlJvmX\"})[counter].find_all(attrs={\"class\": \"ProductCardPrice--bmbnub OBTaB\"})[0].text\n",
    "        except: \n",
    "            the_price_id = soup.find_all(attrs={\"class\":\"ColListing--1fk1zey dlJvmX\"})[counter].find_all(attrs={\"class\": \"ProductCardPrice--bmbnub TGHdV\"})[0].text\n",
    "        the_brand_id = the_name_id.split()\n",
    "        the_brand_id = the_brand_id[0] + \" \" + the_brand_id[1]\n",
    "        the_volume_id = the_name_id.split()\n",
    "        the_volume_id = the_volume_id[-2] + the_volume_id[-1]\n",
    "        \n",
    "        filename = \"IGA Australia\"\n",
    "        fullpath = \"../Output/\" + str(filename) + \".csv\"\n",
    "        #write variables per product into csv file \n",
    "        with open(fullpath, mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            writer.writerow([retailer,the_brand_id, the_volume_id, the_name_id, the_price_id])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9702c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_more_pages(retailer, url):                            \n",
    "                                  \n",
    "    #call get_url function to get the correct url linked to the Region that is going to be scraped    \n",
    "    sleep(1)\n",
    "    \n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "    \n",
    "    driver.get(url)\n",
    "    driver.maximize_window()\n",
    "    sleep(5)\n",
    "    try: \n",
    "        total_results = driver.find_element(By.CSS_SELECTOR, '#Instant\\ Coffee').text\n",
    "        total_results = total_results.replace(\"(\", \"\")\n",
    "        total_results = total_results.replace(\")\", \"\")\n",
    "        total_results = total_results.split()\n",
    "        total_results = total_results[-1]\n",
    "        total_results = int(total_results)\n",
    "    except:\n",
    "        try: \n",
    "            total_results = driver.find_element(By.CSS_SELECTOR, '#Coffee\\ Capsules\\ and\\ Accessories').text\n",
    "            total_results = total_results.replace(\"(\", \"\")\n",
    "            total_results = total_results.replace(\")\", \"\")\n",
    "            total_results = total_results.split()\n",
    "            total_results = total_results[-1]\n",
    "            total_results = int(total_results)\n",
    "        except:\n",
    "            try:\n",
    "           \n",
    "                total_results = driver.find_element(By.CSS_SELECTOR, '#Iced\\ Coffee').text\n",
    "                total_results = total_results.replace(\"(\", \"\")\n",
    "                total_results = total_results.replace(\")\", \"\")\n",
    "                total_results = total_results.split()\n",
    "                total_results = total_results[-1]\n",
    "                total_results = int(total_results)\n",
    "            except:\n",
    "                print(\"doei\")\n",
    "    print(total_results)\n",
    "    \n",
    "        \n",
    "    \n",
    "    lengte = 0\n",
    "    num_products_view_2 = 0\n",
    "    scroll_range = 0\n",
    "     \n",
    "    res = driver.page_source.encode('utf-8')\n",
    "    soup = BeautifulSoup(res, \"html.parser\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    #create attributes the_name_id, the_price_id, the_brand_id and the_volume_id\n",
    "    for _ in range(20):\n",
    "        \n",
    "        res = driver.page_source.encode('utf-8')\n",
    "        soup = BeautifulSoup(res, \"html.parser\")\n",
    "        \n",
    "        # total number of products in current view\n",
    "        num_products_view_1 = int(len(soup.find_all(attrs={\"class\": \"ColListing--1fk1zey dlJvmX\"})))\n",
    "        print(num_products_view_1)\n",
    "        \n",
    "        for counter in range(num_products_view_2, num_products_view_1):\n",
    "            the_name_id = soup.find_all(attrs={\"class\": \"ColListing--1fk1zey dlJvmX\"})[counter].find_all(attrs={\"class\": \"ProductCardTitle--1qhpid2 kugdfA\"})[0].text\n",
    "            the_name_id = the_name_id[:-24]\n",
    "            print(the_name_id)\n",
    "            try: \n",
    "                the_price_id = soup.find_all(attrs={\"class\": \"ColListing--1fk1zey dlJvmX\"})[counter].find_all(attrs={\"class\": \"ProductCardPrice--bmbnub OBTaB\"})[0].text\n",
    "                the_price_id = the_price_id.split()\n",
    "                the_price_id = the_price_id[-1]\n",
    "            except: \n",
    "                the_price_id = soup.find_all(attrs={\"class\":\"ColListing--1fk1zey dlJvmX\"})[counter].find_all(attrs={\"class\": \"ProductCardPrice--bmbnub TGHdV\"})[0].text\n",
    "            print(the_price_id)\n",
    "            the_brand_id = the_name_id.split()\n",
    "            the_brand_id = the_brand_id[0] + \" \" + the_brand_id[1]\n",
    "            the_volume_id = the_name_id.split()\n",
    "            the_volume_id = the_volume_id[-2] + the_volume_id[-1]\n",
    "        \n",
    "            filename = \"IGA Australia\"\n",
    "            fullpath = \"../Output/\" + str(filename) + \".csv\"\n",
    "            #write variables per product into csv file \n",
    "            with open(fullpath, mode='a', newline='', encoding='utf-8') as csv_file:\n",
    "                writer = csv.writer(csv_file)\n",
    "                writer.writerow([retailer,the_brand_id, the_volume_id, the_name_id, the_price_id])\n",
    "            lengte+=1\n",
    "            \n",
    "            scroll_range += 70 \n",
    "            driver.execute_script('window.scrollTo(0, ' + str(scroll_range) + ')')\n",
    "        #break als er geen \"next\" button gevonden kan worden  \n",
    "        try: \n",
    "            \n",
    "            sleep(2)\n",
    "            new_element = driver.find_element(By.CSS_SELECTOR, '#pageMain > div > div > div > div.CategoriesContainer--41glgw.iZUEda > div > section.Pagination--rsd5c4.eNa-DUX > nav > ul > li:nth-child(7)')\n",
    "            new_element.click()\n",
    "        except: \n",
    "            print(\"kan geen nieuwe pagina vinden\")\n",
    "            break\n",
    "        \n",
    "        if lengte >= total_results: \n",
    "            break \n",
    "        sleep(10)\n",
    "           \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a850e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "urls = ['https://new.igashop.com.au/sm/pickup/rsid/52511/categories/coffee/coffee-beans-id-Coffee_Beans',\n",
    "        'https://new.igashop.com.au/sm/pickup/rsid/52511/categories/coffee/roast-and-ground-coffee-id-Roast_and_Ground_Coffee',\n",
    "        'https://new.igashop.com.au/sm/pickup/rsid/52511/categories/coffee/specialty-coffee-id-Specialty_Coffee']\n",
    "urls_more = ['https://new.igashop.com.au/sm/pickup/rsid/52511/categories/coffee/instant-coffee-id-Instant_Coffee', \n",
    "             'https://new.igashop.com.au/sm/pickup/rsid/52511/categories/coffee/coffee-capsules-and-accessories-id-Coffee_Capsules_and_Accessories', \n",
    "             'https://new.igashop.com.au/sm/pickup/rsid/52511/categories/coffee/iced-coffee-id-Iced_Coffee']\n",
    "for i in range(0,len(urls_more)):\n",
    "    scroll_more_pages(\"IGA Australia\", urls_more[i])\n",
    "for i in range(0,len(urls)):\n",
    "    scroll_page(\"IGA Australia\", urls[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b24d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
